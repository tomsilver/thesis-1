\begin{savequote}[75mm]
Don't get fooled by people who claim to have a solution to Artificial General Intelligence... Ask them what error rate they get on MNIST or ImageNet.
\qauthor{Yann LeCun}
\end{savequote}


\chapter{The Luna Rating System}

\section{Evaluating the Intelligence of Machines}

\subsection{Introduction}

Research in artificial intelligence is driven by standardized tests and competitions. The level of success of an algorithm on a widely used test can determine the amount of funding and attention from academia that it receives. With such a burden placed on these tests, one would hope that they provide an accurate reflection of the extent to which an algorithm has achieved artificial intelligence. Unfortunately, existing tests fall far short of this mark \cite{shieber15}. At best, current contests evaluate the performance of specialized algorithms on restricted problem domains that are not guaranteed to generalize, such as object recognition \cite{lecun98, russakovsky15} or robot soccer \cite{anderson11, kitano97}. At worst, tests prematurely claim to be the ultimate arbiter of general intelligence \cite{loebner03}, inspiring a distracting and unwarranted media frenzy when they are passed \cite{anonymous14, shieber14}. In all cases, the current benchmarks for evaluating AI machines are leading the research community away from the creation of truly general intelligence.

In this thesis, I put forth a new system for evaluating general AI. The system invites humans and machines to participate in two-player games in which each player assesses the intelligence of her opponent. The aggregation of these assessments is used to assign a rating to the player. A player's rating is a reflection of the player's demonstrated intelligence. In this sense, the ratings that emerge from the system are the results of a never-ending test of each of the player's general AI. The definition of intelligence is not presupposed in this system; instead, it emerges from the collective judgement of all players. Thus the system is a microcosm of the natural process that humans use to evaluate each other's intelligences. I will argue that the system offers a bright light on the dim path towards machine intelligence.

\subsection{Principles of a Practical Test for Intelligence}

The system proposed in this thesis is meant to steer AI research towards the creation of general intelligence. Therefore, the system must actually be used; a thought experiment will not suffice. Here I present five principles that must be obliged if the test for intelligence is to fulfill its stated purpose.

\subsubsection{Accessibility}
To serve as a useful guide, the test for AI must be constantly accessible to researchers. Ideally the test should be efficient enough that it may be used several times throughout the course of an AI developer's day. This principle discourages a centralized competition that is only held at regular intervals, and instead favors a rating system that can be accessed through the Internet and then carried out using the resources of an average computer.

\subsubsection{Generalizability}
A test need not span all possible areas of intelligence, but the results should reflect the subject's ability to perform in all areas. In this sense, the test should be \textit{AI-complete} --- a machine that does well on this test should do similarly well on any other reasonable test of intelligence. The proposition of the Turing Test, which continues to be held by many researchers, is that the problem domain of natural language is AI-complete. The system proposed here shares this premise.

\subsubsection{Continuity}
Every candidate for AI should be able to observe changes in performance over the course of development. A test that only reports a binary outcome --- pass or fail --- will not be useful for researchers who are not yet close to passing. The test's outcome should instead be continuous, indicating clearly when progress is being made.

\subsubsection{Dependent on People, Independent of Persons}
A fair test should not rely on any one person's interpretation of intelligence. At the same time, intelligence is a social construct that cannot be reasonably appraised without input from humans. Therefore a test for intelligence must take into account the popular conception of intelligence, but it cannot rely on any single human judge to determine its outcome.

\subsubsection{Immunity to Gaming}
It should not be possible for a researcher to ``game the system'' and achieve outsized results by exploiting structure in the test. This principle precludes any sort of hard-coding of knowledge that is \textit{a priori} known to be important for the test. For example, a standardized test fails the Immunity to Gaming principle, since any researcher who observes the results of the test once would be able to submit a machine with the memorized knowledge necessary to pass the test. 

\section{Existing Contests for Artificial Intelligence}

The history of testing AI is long but sparse. It begins in 1950 with the introduction of the Turing Test and meanders into the present day with specialized tests like ImageNet and Robocup. Only recently has there been interest in designing new tests that are more appropriate for practically evaluating general AI \cite{you15}. This interest often manifests as an appeal to move ``beyond the Turing Test'' \cite{marcus14}. However, as I argue below, the Turing Test has never been a practical test for general AI, nor was it ever meant to be. Thus the recent wave of interest is really a call for the first ever practical test for general AI.

\subsection{The Turing Test}

The first formal test for machine intelligence was articulated by Alan Turing, the father of computer science and one of the progenitors of AI \cite{turing50}. Six decades later, the eponymous Turing Test remains at the center of the dialogue surrounding machine intelligence. The Test requires three rooms, each equipped with a telegraph. In one room, the machine candidate for AI is connected to the telegraph, ready to receive and transmit messages. The second room, which is disconnected from the first room, houses a human confederate. A human judge resides in the third room with telegraph connections to the first and second room. The duty of the judge is to determine which room contains the human. Different instantiations of the Test vary details beyond this framework, e.g. how long the conversations go on before a judgment is made \cite{loebner03}. In all cases, the machine is deemed intelligent if it is able to trick the human judge into guessing that it is human.

In applying the principles of a test for intelligence outlined in 1.2, the limitations of the Turing Test come to the fore. The Test is not accessible in practical terms, since it relies on two humans, a judge and a confederate, per machine per test. The Test is generalizable insofar as natural language is AI-complete. The Test is not continuous; it outputs a ``pass'' or ``fail'' depending on whether the participant is able to fool the judge. The Test is very dependent on persons, in particular the human judge and confederate, and does not incorporate any popular conception of intelligence. Finally, the Test is immune to gaming, since it is not possible to anticipate the behavior of the human judge. Thus the Turing Test satisfies only two of the five prescribed principles for a practical test of intelligence.  This analysis is summarized in Table 1. The Test has withstood the test of time for good reason. Its main insight ---  that intelligence cannot be evaluated without an intelligent evaluator --- must be realized by any test designer. Nonetheless, as several have argued \cite{hayes95, moor01, shieber15}, the Turing Test should be viewed as a thought experiment rather than a practical test for intelligence. 

\subsection{Robotics Contests}

Robotics seems like a natural domain for testing artificial intelligence. The prospect of robots that are able to behave and reason at human levels is a clear motivation for AI research. Moreover, a candidate for AI is likely to be more convincing if it is physically instantiated. Anderson, Baltes, and Cheng (2011) review existing robotics contests and critique their utilities as benchmarks for AI research. These contests include annual competitions like AAAI/IJCAI, which consists of a diverse suite of tasks for the robots to perform \cite{balch02}; RoboCup, which requires candidates to play an actual game of soccer, thus entering the realm of multi-agent strategizing \cite{kitano97}; and HuroCup, which might be considered an ``olympics for robots'', requiring contestants to compete in a wide range of sports and agility competitions \cite{baltes08}. Anderson, Baltes, and Cheng find specific shortcomings in each of these competitions as proxies for AI, but suggest that more broad and versatile robotics competitions could still be useful for testing AI.

I argue that a test for AI should be independent of robotics. One inherent problem with robotics as a medium to test AI is that there are a host of extremely challenging problems in the field that have little or nothing to do with intelligence (e.g. the difficult mechanical problems associated with walking \cite{todd13}). It may be that certain problems in robotics are sufficient to demonstrate AI, but those problems are often harder than AI itself. Another fundamental problem in robotics competitions that currently exist is that they consist of a fixed set of a tasks. Even if these tasks are broad, this dependency is a violation of two of my principles for a practical test for AI: Dependent on People/Independent of Persons, and Immunity to Gaming. Any fixed set of tasks is susceptible to be criticized by third parties as unrepresentative of AI. Moreover, a robot can be trained to accomplish this fixed set of tasks without possessing any unifying architecture for general AI. Thus, robotics should be seen not as a medium for testing AI, but rather as an application of AI once it has been reached. Too keen of a focus on robotics threatens to pull research away from the most direct path towards AI. 

\subsection{Other AI Competitions}

There are several other existing competitions and prizes for artificial intelligence. Some of these take the form of general awards for career achievement, such as the annual David E. Rumelhart prize \cite{rumelhart} and the IJCAI Award for Research Excellence \cite{ijcai}. These awards serve to encourage research in AI, but do not claim to test AI nor directly steer the field towards it. Other contests focus on specialized tasks. ImageNet requires contestants to recognize objects in a massive image dataset \cite{russakovsky15}. The Hutter Prize offers a benchmark for lossless text compression \cite{mahoney09}. Several contests focus on AI game playing, such as chess \cite{hayes76}, poker \cite{littman06}, and general game playing \cite{genesereth05}. Few of these competitions claim to test general AI, and none are successful in satisfying all five principles of a practical test for AI (see Table 1). The search for a practical test for AI continues. 

\section{The Luna Rating System}

\subsection{Intelligence as a Social Construct}

Psychology and artificial intelligence offer several competing definitions of intelligence. Legg and Hutter collect 70 distinct definitions from both fields, admitting that even this list is incomplete \cite{legg07}. A unifying definition would be highly convenient for the purpose of designing a test. Alas, any one definition is revealed to be incomplete or flawed as soon as it is written down. Why is a definition for intelligence, a property with obvious societal importance, so elusive? I argue that intelligence is not an immutable property found in nature, but rather a social construct that depends completely on the society that constructs it. If humans did not exist, intelligence would not exist. 

Given this premise, it is clear that any test for artificial intelligence that does not receive direct input from human judges will be inherently flawed. A true test must capture the socially constructed definition of intelligence. In an ideal scenario, a machine would be evaluated by virtually every member of society using whatever methods of evaluation  each individual finds appropriate. A practical test must work to the same end under realistic constraints. Here I introduce the Luna Rating System as such a practical test and propose that it is an authentic reflection of the socially constructed notion of intelligence.

\subsection{The Luna Rating System}

The Luna Rating System (LRS) is a never-ending tournament of a two-player game called the Luna Game. Players of the Luna Game may be human or machine. All players have two consistent goals: to accurately evaluate the intelligence of their opponents, and to prove their own intelligence. Players are driven to demonstrate intelligence because they want to achieve a high \textit{Smarts Rating}. The Smarts Rating is the central quantity of interest for the purpose of identifying artificial intelligence. If a machine achieves a Smarts Rating that is on par with the Smarts Ratings of humans, it will have passed the implicit intelligence test of LRS and convincingly demonstrated general AI.

The details of the Luna Game are covered in Chapter 2. For the purpose of introducing LRS, it suffices to assume that a Luna Game requires each of the two players to evaluate the other's intelligence. A player's Smarts Rating is a reflection of the extent to which that player has convinced her past Luna Game opponents that she is intelligent. At the end of a Luna Game, a player's Smarts Rating is updated based on the evaluation of the opponent via Kalman filtering \cite{julier97}. Thus every player in LRS is constantly a judge of intelligence and a candidate under evaluation by other players.

One immediate advantage of LRS is its strong incentive for judges to perform their duties well. No other testing system for AI with human judges has a built-in impetus for high quality judging. Moreover, as the details of the Luna Game make clear, judges are not perversely motivated to attempt to fool candidates for AI by putting forth excessively difficult tests; instead, they are rewarded for giving judgments that are close to the consensus of all players in the system, and therefore close to the socially constructed definition of intelligence. 

\subsection{Analysis of Principles}

LRS is the only practical test for intelligence to satisfy all five of the principles outlined in 1.2. The system exists online and invites humans and machines to play for free. Thus LRS is maximally accessible. Like the Turing Test, the Luna Game involves open-domain question answering, and therefore is AI-complete, i.e. generalizable to all problems in AI. Smarts Ratings are continuous quantities that are updated after every Luna Game, making progress immediately clear to all players. These ratings reflect the equilibrium consensus of all players in LRS on what it means to be intelligence, and is not biased towards any single player's notion. Finally, the system is immune to gaming, since all players are encouraged to devise their own original questions, which cannot be predicted by the other player. Table 1 summarizes how the satisfaction of these five principals represents a substantial improvement over existing tests for AI.

\begin{table}[h!]
\centering
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{Principle}                                                                              & \textbf{Turing} & \textbf{ImageNet} & \textbf{HuroCup} & \textbf{Hutter} & \textbf{Games} & \textbf{LRS} \\ \hline
Accessibility                                                                          &        & X        &         & X      & X     & X   \\ \hline
Generalizibility                                                                       & X      &          & X       &        &       & X   \\ \hline
Continuity                                                                             &        & X        & X       & X      & X     & X   \\ \hline
\begin{tabular}[c]{@{}l@{}}Dependent on People, \\ Independent of Persons\end{tabular} &        &          &         &        &       & X   \\ \hline
Immunity to Gaming                                                                     & X      &          &         &        & X     & X   \\ \hline
\end{tabular}
\caption{\textbf{The Luna Rating System is the only test that satisfies all five principles for a practical test of intelligence.} Other existing tests include the Turing Test, which invokes natural language; ImageNet, which focuses on object recognition in computer vision; HuroCup, a sports-based competition in robotics; the Hutter Prize, which deals with text compression; and several games, such as chess, poker, and general game playing.}
\end{table}

\subsection{Thesis Outline}

The primary contribution of this thesis is the introduction of the Luna Rating System as a practical test for machine intelligence. The remainder of the thesis is divided into three parts. In the next chapter, I continue the introduction of the Luna Rating System through a description of the Luna Game. This chapter is followed by a study of the robustness of LRS, characterizing likely strategies for game play and analyzing their effect on the accuracy of Smarts Ratings as a proxy for intelligence. In Part II, I enter LRS as a machine player. The Luna Game is broken down into three separate problems in natural language processing and machine learning. Each is given theoretical treatment and several novel solutions are proposed, many of which generalize beyond the scope of LRS. Finally, in Part III, I create the first online instantiation of LRS and invite humans and machines to play. Their games illuminate the current state of AI and provide a rich natural language dataset for further analysis.

%\pagebreak
%\bibliographystyle{abbrvnat}
%
%\begin{thebibliography}{}
%
%\bibitem{anderson11}
%Anderson, John, Jacky Baltes, and Chi Tai Cheng. ``Robotics competitions as benchmarks for AI research.'' The Knowledge Engineering Review 26.01 (2011): 11-17.
%
%\bibitem{anonymous14}
%Anonymous. ``Computer Simulating 13-year-old Boy Becomes First to Pass Turing Test.'' Http://www.theguardian.com/. The Guardian, 9 June 2014. Web.
%
%\bibitem{balch02}
%Balch, T. \& Yanco, H. 2002. ``Ten years of the AAAI mobile robot competition and exhibition.'' AI Magazine 23(1), 13?22.
%
%\bibitem{baltes08}
%Baltes, Jacky, and Thomas Braunl. ``HUROCUP: General Laws of the Game 2008.'' (2009).
%
%\bibitem{genesereth05}
%Genesereth, Michael, Nathaniel Love, and Barney Pell. ``General game playing: Overview of the AAAI competition.'' AI magazine 26.2 (2005): 62.
%
%\bibitem{hayes76}
%Hayes, Jean E., and David NL Levy. The world computer chess championship, Stockholm 1974. Edinburgh University Press, 1976.
%
%\bibitem{hayes95}
%Hayes, Patrick, and Kenneth Ford. ``Turing test considered harmful.'' IJCAI (1). 1995.
%
%\bibitem{ijcai}
%``IJCAI Award for Research Excellence.'' International Joint Conferences on Artificial Intelligence. N.p., n.d. Web. 12 Dec. 2015.
%
%\bibitem{julier97}
%Julier, Simon J., and Jeffrey K. Uhlmann. ``New extension of the Kalman filter to nonlinear systems.'' AeroSense'97. International Society for Optics and Photonics, 1997.
%
%\bibitem{kitano97}
%Kitano, Hiroaki, et al. "Robocup: The robot world cup initiative." Proceedings of the first international conference on Autonomous agents. ACM, 1997.
%
%\bibitem{lecun98}
%LeCun, Yann, Corinna Cortes, and Christopher JC Burges. ``The MNIST database of handwritten digits.'' (1998).
%
%\bibitem{legg07}
%Legg, Shane, and Marcus Hutter. ``A collection of definitions of intelligence.'' Frontiers in Artificial Intelligence and applications 157 (2007): 17.
%
%\bibitem{littman06}
%Littman, Michael, and Martin Zinkevich. ``The 2006 AAAI computer poker competition.'' ICGA Journal 29.3 (2006): 166.
%
%\bibitem{loebner03}
%Loebner, Hugh. ``Home page of the Loebner prize-the first Turing test.'' Online unter http://www. loebner. net/Prizef/loebner-prize. html (2003).
%
%\bibitem{mahoney09}
%Mahoney, Matt. ``Rationale for a Large Text Compression Benchmark.'' Rationale for a Large Text Compression Benchmark. N.p., 23 July 2009. Web. 12 Dec. 2015.
%
%\bibitem{marcus14}
%Marcus, Gary. ``What Comes After the Turing Test?'' The New Yorker. The New Yorker, 09 June 2014. Web. 12 Dec. 2015.
%
%\bibitem{moor01}
%Moor, James H. ``The status and future of the Turing test.'' Minds and Machines 11.1 (2001): 77-93.
%
%\bibitem{rumelhart}
%``For Contributions to the Theoretical Foundations of Human Cognition.'' The David E Rumelhart Prize RSS. N.p., n.d. Web. 12 Dec. 2015.
%
%\bibitem{russakovsky15} 
%Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.
%
%\bibitem{shieber14}
%Shieber, Stuart M. No, the Turing Test has not been passed. In The Occasional Pamphlet. 10 June 2014a. URL http://blogs.law.harvard.edu/pamphlet/2014/06/ 10/no-the-turing-test-has-not-been-passed/.
%
%\bibitem{shieber15}
%Shieber, Stuart M. ``Principles for Designing an AI Competition, or Why the Turing Test Fails as an Inducement Prize'' Forthcoming. (2015).
%
%\bibitem{todd13}
%Todd, David J. Walking machines: an introduction to legged robots. Springer Science \& Business Media, 2013.
%
%\bibitem{turing50}
%Turing, Alan M. ``Computing machinery and intelligence.'' Mind (1950): 433-460.
%
%\bibitem{you15}
%You, Jia. ``Beyond the Turing Test.'' Science 347.6218 (2015): 116-116.
%
%%\end{thebibliography}
%
%\end{document}