\begin{savequote}[75mm]
Don't get fooled by people who claim to have a solution to Artificial General Intelligence... Ask them what error rate they get on MNIST or ImageNet.
\qauthor{Yann LeCun}
\end{savequote}


\chapter{The Luna Rating System}

\section{Introduction}

On March 12, 2016, just after 5 pm in Seoul, Lee Se-dol, the 18-time world champion of the board game Go, conceded defeat. His opponent had trounced him in three consecutive games, stunning him with unexpected moves and overwhelming him with a demonstrated mastery of the game's finest details. The man who placed the winning stones onto the Go board swiftly stepped aside to reveal the series' true victor: an Artificial Intelligence (AI) developed by Google DeepMind called AlphaGo \citep{silver2016mastering}. The outcome was heralded internationally as a revolutionary achievement for AI. ``AlphaGo's victory means the world is about to change,'' proclaimed \textit{The Next Web} \citep{1_nextweb_2016}. ``Artificial intelligence [has come] of age in showdown between human brainpower and a machine,'' \textit{the Guardian} similarly declared \citep{1_the_guardian_2016}. 

To long-time observers of AI research, these headlines likely look familiar. The sensation is similar to the coverage of Eugene Goostman's so-called passing of the Turing Test --- ``a milestone in artificial intelligence,'' said \textit{the Guardian} \citep{1_the_guardian_2014, occasional_pamphlet_2014} --- and the wave of hype surrounding Watson's victory in Jeopardy! --- ``a vindication for the academic field of artificial intelligence,'' reported \textit{The New York Times} \citep{1_newyorktimes_2011}. In these cases and others, after the dust settles, the significance of the achievement in the context of general AI research remains unclear. Expert views on the current state of AI could not be more varied or discordant. One representation of this disarray is the recent collection of essays on \textit{What to Think About Machines that Think}, in which over 200 leading thinkers disagree on the definition, timeline, and even theoretical possibility of AI  \citep{edge2016what}. There is no shortage of speculation, nor any sign of consensus, on the emergence of truly intelligent machines.

Day-to-day research in artificial intelligence proceeds unencumbered by these speculations. A model's worth is not measured by public perception but rather, according to its performance on benchmarks of the field. Such benchmarks --- e.g. object recognition in ImageNet  \citep{russakovsky2015imagenet}, language modeling in the 1 Billion Word Dataset  \citep{chelba2013one}, reinforcement learning for various Atari Games \citep{mnih2013playing} --- are attractive for researchers who are seeking measurable, specific improvements on the state of a particular art. These benchmarks can have tremendous influence on the direction of AI research. In the best case, they can provide cohesion and order to a burgeoning field. Unfortunately, they can also incentivize incremental progress on narrow problems and effectively discourage large leaps of innovation \citep{shieber2015}. Moreover, these benchmarks give researchers little insight as to a model's proximity to AI. A solution to a specialized task may not generalize beyond the scope of the task. Apparent progress on a particular task --- object recognition, chess, Go, etc. --- may actually be negligible in the long arc of AI research.

In this thesis, I present an original benchmark for AI, arguably the first of its kind. The benchmark, which I call Luna\footnote{Luna is a tribute to Alan Turing; Luna Rating is an anagram of Alan Turing.}, takes inspiration from the network-based rating systems of chess and the natural language interrogations of the Turing Test. Luna invites humans and machines to participate in two-player games in which each player assesses the intelligence of the other. The aggregation of these assessments is used to assign a rating to the player. The ratings that emerge from the system are the results of a never-ending test of each player's AI. The definition of intelligence is not presupposed in this system; instead, it emerges from the collective judgment of all players. Thus, the system is a microcosm of the natural process that humans use to evaluate each other's intelligences. I will argue that the system offers a bright guiding light on the dim path towards machine intelligence.

\section{Existing Tests for AI}

The history of testing AI is long but sparse. It begins in 1950 with the introduction of the Turing Test and meanders into the present day with a focus on specialized tasks. Only recently has there been interest in designing new tests that are more appropriate for practically evaluating AI  \citep{you2015beyond}. This interest often manifests as an appeal to move ``beyond the Turing Test''  \citep{1_the_newyorker_2015}. However, the Turing Test has never been a practical test for AI. Thus the recent wave of interest is really a call for the first ever practical test for AI.

\subsection{The Turing Test}

The first formal test for machine intelligence was articulated by Alan Turing, the father of computer science and one of the progenitors of AI  \citep{turing1950computing}. Six decades later, the eponymous Turing Test remains at the center of the dialogue surrounding machine intelligence. The Test requires three rooms, each equipped with a telegraph. In one room, the machine candidate for AI is connected to the telegraph, ready to receive and transmit messages. The second room, which is disconnected from the first room, houses a human confederate. A human judge resides in the third room with telegraph connections to the first and second room. The duty of the judge is to determine which room contains the human. Different instantiations of the Test vary details beyond this framework, e.g. in how long the conversations go on before a judgment is made  \citep{loebner2003home, bishop2010testing}. In all cases, the machine is deemed intelligent if it is able to trick the human judge into guessing that it is human.

Since its proposal, the Turing Test has weathered criticism from every conceivable angle. There are myriad philosophical objections, especially those of Block, Gunderson, and Searle, who argue that intelligence is by definition a capacity, and that the Test cannot prove the existence of capacity   \citep{block1980intuitions, gunderson1964vii, searle1980minds}. Shieber saves the Test from this line of critique using Interactive Proofs as a metaphor  \citep{shieber2007turing}, but maintains that the Test should be viewed as a thought experiment, rather than a practical inducement for research  \citep{shieber2015}. Hayes and Ford agree with the impracticality of the Test  \citep{hayes1995turing}. They go so far as to call the Test ``harmful'' for AI research, citing the lack of restrictions placed on judges, the emphasis on deception, and the binary result as fundamental failures of the Test. Indeed, attempts at practical instantiations of the Test (e.g. the Loebner Prize) have been met with strong criticism from the research community  \citep{shieber1994lessons, occasional_pamphlet_2014}. The Test remains synonymous with AI in the public parlance, but researchers are increasingly looking ``beyond the Turing Test'' for more practical tests of AI  \citep{1_the_newyorker_2015, you2015beyond}.

\subsection{Robotics Contests}

Robotics seems like a natural domain for testing artificial intelligence. The prospect of robots that are able to behave and reason at human levels is a clear motivation for AI research. Moreover, a candidate for AI is likely to be more convincing if it is physically instantiated. Anderson, Baltes, and Cheng (2011) review existing robotics contests and critique their utilities as benchmarks for AI research. These contests include annual competitions like AAAI/IJCAI, which consists of a diverse suite of tasks for the robots to perform  \citep{balch2002ten}; RoboCup, which requires candidates to play an actual game of soccer, thus entering the realm of multi-agent strategizing  \citep{kitano1997robocup}; and HuroCup, which might be considered an ``olympics for robots'', requiring contestants to compete in a wide range of sports and agility competitions  \citep{baltes2009hurocup}. Anderson, Baltes, and Cheng find specific shortcomings in each of these competitions as proxies for AI, but suggest that more broad and versatile robotics competitions could still be useful for testing AI.

I argue that a test for AI should be independent of robotics. One inherent problem with robotics as a medium to test AI is that there are a host of extremely challenging problems in the field that have little or nothing to do with intelligence (for example, the difficult mechanical problems associated with walking). It may be that certain problems in robotics are sufficient to demonstrate AI, but those problems are often harder than AI itself. Another fundamental problem in existing robotics competitions is that they consist of a fixed set of a tasks. Any fixed set of tasks is susceptible to be criticized by third parties as unrepresentative of AI. Moreover, a robot can be trained to accomplish this fixed set of tasks without possessing any unifying architecture for AI. Thus, robotics should be seen not as a medium for testing AI, but rather as an application of AI once it has been reached. Too keen of a focus on robotics threatens to pull research away from the most direct path towards AI. 

\subsection{Specialized Benchmarks}

The overwhelming majority of testing in modern AI research is performed on specialized benchmarks. ImageNet, which requires contestants to recognize objects in a massive image dataset, is a well known example  \citep{russakovsky2015imagenet}. Various Natural Language Processing problems, such as Part of Speech Tagging and Language Modeling, are often studied using the Penn Treebank dataset as a benchmark  \citep{marcus1993building}. Mnih et al. establish a suite of Atari games as a benchmark for reinforcement learning  \citep{mnih2013playing}. Several contests focus on AI game playing, such as chess  \citep{hayes1976world}, poker  \citep{littman20062006}, and general game playing  \citep{genesereth2005general}. The Hutter Prize offers a benchmark for lossless text compression  \citep{mahoney2006rationale}. None of these competitions claim to test general AI. The search for a practical test for AI continues. 

\section{A Pragmatic Sufficiency for Intelligence}

Given the substantial and sustained research attention that AI has enjoyed, the lack of a unified practical benchmark may be rather surprising. This absence can be partly explained by the lack of consensus surrounding the definition of intelligence. Psychology and AI suggest several definitions of the term. Legg and Hutter collect 70 distinct definitions from both fields, admitting that even this list is incomplete  \citep{legg2007collection}. To further complicate matters, intelligence is also entangled with other philosophical quandaries, such as the nature of consciousness. The Turing Test sidesteps the definition problem by suggesting a sufficient condition without claiming that the condition is also necessary; passing the Test is enough to demonstrate intelligence, but failing does not prove its absence. However, as described above, even this sufficient condition is vulnerable to philosophical criticism. It seems as though any proposed definition or sufficiency for intelligence will invariably be contested.

In search of a path around this philosophical obstacle, I begin with a tautological observation: a machine would be called intelligent if everyone called it intelligent. If the machine simply generated random sequences of letters and was able to ``feign intelligence'' by random luck, it would still be called intelligent. (In the same way, a human randomly guessing on an IQ test could be deemed intelligent by chance, or a non-Italian speaker visiting Italy could feign fluency by randomly blurting out syllables and getting lucky.) Such a randomly lucky machine would be evidence of weak AI, but not of strong AI, to use the terminology of Searle: weak AI requires only the apparent simulation of a mind, while strong AI insists that a machine \textit{is} a mind  \citep{searle1980minds}. For the purpose of developing a practical test, measuring weak AI suffices. Russell and Norvig, authors of the most widely cited textbook on AI, confirm this view, remarking that ``most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis''  \citep{russell1995modern}. If a machine is able to convincingly demonstrate intelligence to everyone in the world, surely that would be sufficient to call it weak AI.

Of course, the requirement that everyone weigh in on the intelligence status of a machine is impractical and overly strict. At the same time, a benchmark relying on human judgments must be sufficiently standardized so that subjective variations do not prevent meaningful comparisons between test instances. And yet standardization threatens to impose an implicit definition of intelligence. To resolve this paradox, I take inspiration from the rating systems of chess  \citep{glickman1995chess}. The rating assigned to a chess player is meant to reflect that player's ability relative to all other rated players. These ratings are not assigned based on complete tournaments in which every player challenges every other player; rather, the ratings are awarded based on the individual outcomes of two-player games. Each game is seen as a sample of the player's ability, and ratings are updated after each game to take into account this new sample. Thus the ratings are constantly moving towards true representations of the relative abilities of all players. Chess rating systems are able to assign a rating to a player that reflects the consensus of all players in the system, without relying on every player for every rating.

With chess rating systems and the Turing Test in mind, I propose the following pragmatic sufficiency for AI: \textbf{if human consensus deems the verbal responses of a machine to be evident of intelligence comparable with that of most other humans, then the machine is intelligent.}  Of course, this sufficiency as stated leaves many questions unanswered. How many humans must directly provide input in order to capture consensus? What precisely is meant by ``comparable'' and ``most''? These are questions that may be addressed philosophically or empirically. But for the purpose of developing a benchmark, it suffices to measure relative progress: if human consensus deems the verbal responses of one machine to be more evident of intelligence than those of another machine, then the former is closer to achieving AI. Similarly, as the number of humans providing direct inputs increases, accuracy increases, as the collective judgment will converge towards the consensus. This pragmatic sufficiency for intelligence forms the foundation of the Luna Rating System, my proposed benchmark for AI.

\section{The Luna Rating System}

\subsection{The Luna Rating System}

The Luna Rating System (LRS) is a never-ending tournament of a two-player game called the Luna Game. The details of the Luna Game are covered in Chapter 2. For the purpose of introducing LRS, it suffices to say that a Luna Game requires each of the two players to guess the intelligence, or Smarts Rating, of the other player. The winner of the game is the player whose guess is closest to the actual rating. At the end of a Luna Game, a player's Smarts Rating is updated based on the opponent's guess. Players are encouraged to maximize their own Smarts Ratings and their number of game wins. They are therefore incentivized to demonstrate maximal intelligence and to judge the intelligence of other players with maximal accuracy. The symmetry of the Luna Game is intentional: every player in LRS is constantly a judge of intelligence and a candidate under evaluation by other players.

In benchmarking AI, the central quantity of interest in LRS is a player's Smarts Rating. This number encapsulates the human consensus of the intelligence of that player. If a machine achieves a Smarts Rating that is on par with the Smarts Ratings of humans, after a sufficient number of Luna Games, it will have passed the implicit intelligence test of LRS and convincingly demonstrated AI. Moreover, the effect of changes made to the machine may be measured according to the subsequent difference in Smarts Rating. The Smarts Rating is the number that should be reported in characterizing the success of a candidate for AI.

One immediate advantage of LRS is its strong incentive for judges to perform their duties well. No other testing system for AI with human judges has a built-in impetus for high quality judging. Moreover, as the details of the Luna Game make clear, judges are not perversely motivated to attempt to fool candidates for AI by putting forth excessively difficult tests; instead, they are rewarded for giving judgments that are close to the consensus of all players in the system. Another critical advantage of LRS is the continuity of Smarts Ratings. As opposed to binary systems like the Turing Test, progress, however small, is observable. This property distinguishes LRS as an informative benchmark, rather than a theoretical test. An additional feature of LRS is its accessibility. The system is meant to be freely and constantly available to researchers, e.g. through the World Wide Web. These characteristics and others significantly differentiate LRS from existing tests for AI. Moreover, as I argue in the following section, LRS is unique in satisfying all of the fundamental principles of a practical test for intelligence.

\subsection{Principles of a Practical Benchmark for AI}

Here I propose five principles that must be obliged if a benchmark for AI is to fulfill its stated purpose. This exercise is inspired by the principles put forth in \citet{shieber2015} for an AI inducement prize. Shieber's proposed criteria apply to organized contests in which contestants compete on a shared task for a prize. For example, his second principle suggests that ``the awarding process should be \textit{flexible}, so awards follow the spirit of the competition rather than the letter of the rules.'' These inducement prize principles do not directly apply to benchmarks, hence the need to develop principles for the latter. The principles suggested below are meant to complement those for inducement prizes; together, the two sets of principles form a complete blueprint for AI evaluation.

\subsubsection{Accessibility}
To serve as a useful guide, the benchmark for AI must be constantly accessible to researchers. Ideally the test should be efficient enough that it may be used several times throughout the course of an AI developer's day. This principle discourages a centralized competition that is only held at regular intervals, and instead favors a system that can be accessed through the Internet and then carried out using the resources of an average computer.

\subsubsection{Generalizability}
A benchmark need not span all possible areas of intelligence, but the results should reflect the subject's ability to perform in all areas. In this sense, the test should be \textit{AI-complete} --- a machine that does well on this test should do similarly well on any other reasonable test of intelligence. The proposition of the Turing Test, which continues to be held by many researchers, is that the problem domain of natural language is AI-complete \citep{yampolskiy2013turing, weston2015towards}. The system proposed here shares this premise.

\subsubsection{Continuity}
Every candidate for AI should be able to observe changes in performance over the course of development. A benchmark that only reports a binary outcome --- pass or fail --- will not be useful for researchers who are not yet close to passing. The benchmark's outcome should instead be continuous, indicating clearly when progress is being made.

\subsubsection{Independent of Persons}
A fair benchmark should not rely on any one person's interpretation of intelligence. The variety of definitions of intelligence within the research community, let alone throughout the general public, demonstrates the importance of this principle. A benchmark that relies on a single human judge would clearly be vulnerable to subjectivity and error.

\subsubsection{Immunity to Gaming}
It should not be possible for a researcher to ``game the system'' and achieve outsized results by exploiting structure in the test. This principle precludes any sort of hard-coding of knowledge that is \textit{a priori} known to be important for the test. For example, a standardized test fails the Immunity to Gaming principle, since any researcher who observes the results of the test once would be able to submit a machine with the memorized knowledge necessary to pass the test. 

\subsection{Analysis of Principles}

LRS is the only test for intelligence to satisfy all five of the principles outlined above. The system exists online and invites humans and machines to play for free. Thus LRS is maximally accessible. Like the Turing Test, the Luna Game involves open-domain question answering, and therefore is AI-complete, i.e. generalizable to all problems in AI. Smarts Ratings are continuous quantities that are updated after every Luna Game, making progress immediately clear to all players. These ratings reflect the equilibrium consensus of all players in LRS on what it means to be intelligence, and is not biased towards any single player's notion. Finally, the system is immune to gaming, since all players are encouraged to devise their own original questions, which cannot be predicted by the other player. Table \ref{principles} summarizes how the satisfaction of these five principals represents a substantial improvement over existing tests for AI.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{Principle}                                                                              & \textbf{Turing} & \textbf{ImageNet} & \textbf{HuroCup} & \textbf{Hutter} & \textbf{Games} & \textbf{LRS} \\ \hline
Accessibility                                                                          &        & \checkmark        &         & \checkmark      & \checkmark     & \checkmark   \\ \hline
Generalizibility                                                                       & \checkmark      &          & \checkmark       &        &       & \checkmark   \\ \hline
Continuity                                                                             &        & \checkmark        & \checkmark       & \checkmark      & \checkmark     & \checkmark   \\ \hline
\begin{tabular}[c]{@{}l@{}}Independent of Persons\end{tabular} &        &          &         &        &       & \checkmark   \\ \hline
Immunity to Gaming                                                                     & \checkmark      &          &         &        & \checkmark     & \checkmark   \\ \hline
\end{tabular}
\caption{\label{principles}\textbf{The Luna Rating System is the only test that satisfies all five principles for a practical test of intelligence.} Other existing tests include the Turing Test, which invokes natural language; ImageNet, which focuses on object recognition in computer vision; HuroCup, a sports-based competition in robotics; the Hutter Prize, which deals with text compression; and several games, such as chess, poker, and general game playing.}
\end{table}

\subsection{Contrary Views on the Main Question}

In Turing's seminal paper introducing his Test, he includes a section titled ``Contrary Views on the Main Question,'' in which he enumerates and responds to anticipated objections  \citep{turing1950computing}. Here I follow his lead, briefly considering possible objections to the Luna Rating System.

\subsubsection{Test Time Objection}

Given the reliance on human input, some may object that the time required to obtain meaningful results from LRS is too long. In practice, with a sufficient number of active participants, it should be possible to solicit tens of judgments per day. This timescale is actually far shorter than possible alternatives, especially when those alternatives require humans convening and thus may only occur once every several months or years. Even specialized tasks that do not require any human input can take several hours to return results. Of course, the test time may dramatically increase if human participation is lacking; this condition is addressed in the following objection.

\subsubsection{System Maintenance Objection}

Some may object that a system requiring such broad and consistent human participation is simply too expensive to maintain. In the case that the system is web-based, the standard technical difficulties associated with hosting a website must be addressed. Furthermore, participants must be recruited and persuaded to play. In Chapter 6, I present a proof-of-concept to address this objection, demonstrating that a web-based implementation of LRS can be launched and sustained with relatively limited cost. The success of this simple system suggests that a system with more explicit incentives, e.g. paying participants to play, would have no trouble sustaining the required levels of activity.

\subsubsection{Barrier to Entry Objection}

After reviewing the details of the Luna Game (presented in Chapter 2), some may object that the entry into LRS requires too much additional work on the part of AI researchers. I respond to this objection in Chapter 4 with a review of the extensive body of work on two longstanding AI problems: Question Generation and Question Answering. I demonstrate that entry into LRS requires negligible effort beyond addressing these two problems. Moreover, an approach to the relatively lesser-known problem of Question Generation is actually optional; researchers may manually select a fixed set of questions without affecting the integrity of the test. Question Answering is an extremely active research area and LRS invites direct application of these efforts.

\subsubsection{Definition Drift Objection}

Some may object that Smarts Ratings might drift towards a specific notion that may or may not be related to intelligence. This could occur if players were primed with the suggestion to ask mathematical questions or trivia questions, for example. To prevent this drift, I insist that LRS provides minimal instructions to players as to what is meant by ``intelligence''. Beyond this lack of bias, the integrity of Smarts Ratings is contingent upon the assumption that players will behave according to their own intuitive notions of intelligence. The results presented in Chapter 6 corroborate the reasonableness of this assumption. Additional anecdotal evidence suggests that players enjoy the challenge of creating probing and diverse questions in the course of the game.

\subsection{Single Dimension Objection}

While Smarts Ratings are more expressive than binary outputs, some may object that a single dimension is insufficient to meaningfully capture intelligence. For example, the theory of multiple intelligences from psychology posits that intelligence is best described along eight or more axes \citep{gardner2011frames}. The proposed existence of the $g$ factor, a variable that has been shown to correlate with most other modules of intelligence, is one possible response to this objection  \citep{visser2006g}. More generally, the conception of intelligence as a real value in a single dimension is motivated by the same practicality that underpins much of LRS; such a metric is the simplest way to capture progress or regress in the pursuit of AI.

\subsubsection{Malicious Strategizing Objection}

With a particular weariness of anonymous online players, some may object that malicious strategizing could threaten the integrity of the test. Indeed, if all players decide to behave randomly, the results output by LRS will not be meaningful. As discussed in Chapter 3, LRS does not rely on the honest intentions of all players, nor does it assume that players will always play to the best of their abilities. The one necessary assumption is that the majority of players will ``guess honestly''. The meaning of this requirement is revealed in Chapter 2, and the extent to which the system is vulnerable to malicious guessing is rigorously analyzed in Chapter 3.

\section{Thesis Outline}
The primary contribution of this thesis is the introduction of the Luna Rating System as a practical benchmark for machine intelligence. The remainder of the thesis is roughly divided into three parts. In the next chapter, I continue the introduction of LRS with a description of the Luna Game. This chapter is followed by a study of the robustness of LRS, characterizing likely strategies for game play and analyzing their effect on the accuracy of Smarts Ratings as a proxy for intelligence. Next I characterize the three subproblems that constitute the full Luna Game. The first two problems --- Question Generation and Question Answering --- have been extensively studied in previous work, which I review in Chapter 4. The third subproblem, which I deem Luna Rating Prediction, has not been previously characterized. In Chapter 5, I formalize the problem, argue its merit as a general problem of interest, and present baseline results. Finally, in Chapter 6, I create the first online instantiation of LRS and recruit human participants to play. Their games illuminate the current state of AI and offer rich insight into the human conception of intelligence.
