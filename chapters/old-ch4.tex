\begin{savequote}[75mm]
You are like my father in some ways.\\
WHAT RESEMBLANCE DO YOU SEE\\
You are not very aggressive but I think you don't want me to notice that.\\
WHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE
\qauthor{A conversation with ELIZA (from Weizenbaum 1966)}
\end{savequote}
\chapter{Interview Question Generation}

\section{Introduction}

Applications of artificial intelligence are often reactive: the human poses a question, and the machine attempts an answer. The beginning of a Luna Game may be confounding for such a responsive machine, since the Interview Phase requires each player to provide questions without any prior input. As a test for AI, the Luna Rating System does not demand that a machine be able to ask ``good'' questions; Smarts Ratings are presumably only a function of responses. Researchers who wish to use LRS purely as a test may choose a simple interviewing scheme, such as selection from a fixed set of questions. However, for those who want to take a more holistically AI approach to the Luna Game, the Interview Phase offers an application of the intriguing \textit{Question Generation} (QG) task.

Question Generation demands more than the creation of arbitrary linguistically valid questions. The defining feature of QG is that the questions produced must pertain to a given topic, which is often formalized as a collection of natural language samples. A slightly stronger requirement, sometimes included to narrow the scope of the task, is that the generated questions must be directly answered within the provided text. These text samples may be relatively short, like a section of a Wikipedia article, or long, like the full Wikipedia corpus. They may be factual, like the encyclopedia examples, or fictional, like a short story derived from a simulated world. A method for QG must accept these samples as input and produce questions related to the text. The metric used to assess this output may also vary. A straightforward approach is to count the number of unique questions generated from the input text. This metric avoids the difficulty of defining question quality but subsequently provides no guarantees about the question's content. An alternative method relies on manual human input to determine the quality of each question. While potentially more meaningful, this metric can be costly, subjective, and vulnerable to human error. The challenge of defining a metric that is both semantically meaningful and efficiently evaluated is arguably the greatest barrier between QG and mainstream NLP research.

In this chapter, I present a novel approach to QG that can provide quality questions without human labeling. The method extends the \textit{overgenerating transformations and ranking} paradigm, which begins with the generation of a vast surplus of questions, and then refines the crop by ranking the best questions and selecting from the top. In this context, the limiting factor for progress is the ranking part of the problem; methods for generating semantics-agnostic questions from templates are abundant. Previous work on ranking has relied on human-labelled questions and thus has been limited by the size of available annotated datasets. Taking inspiration from the distributional hypothesis of linguistics, and especially the work of C\&W, I provide a simple method for converting a dataset of unlabeled questions into an augmented dataset of real coherent questions and fake incoherent questions. With this dataset in hand, one can proceed with binary classification methods to train a classifier for question coherence. In particular, methods that require large amounts of training data, such as deep neural networks, become available to the QG task for the first time. 

\section{Related Work}

Question Generation is often motivated by the prospect of automated intelligent educational tools. A system capable of generating a set of reasonable questions from an academic text would be invaluable for learning and assessment, especially in the era of MOOCs and other online educational resources. Heilman's work, which I describe in more detail below, is primarily motivated by this educational potential. In the specific context of English language learning, Kunichika et al. (2004) create an interactive system that generates questions from a novice textbook and then adaptively presents a series of questions to students based on their previous responses. Other motivations for QG include assisting human questioning, participating in general dialogue, and providing synthetic datasets for the related task of Question Answering (see Chapter 5). Piwek et al. (2008) make a broad case for QG as a task of interest for AI and Computational Linguistics, advocating an ``open-minded approach... [towards] a new and hopefully soon burgeoning area of research.''

If surface-level questions suffice for an application, and questions need not be diverse in style, there are several simple QG options available. One coarse approach is the Cloze procedure, in which words from the input text are randomly replaced with blanks for the responder to complete. For example, the input text ``George Washington was born in Virginia'' might generate the question ``George Washington was born in $\rule{2cm}{0.05mm}$''. The next level of sophistication involves applying templates to text in search of specific syntax patterns, and then applying one of several predefined transformations accordingly. For example, the previous input text might be matched and transformed to the question ``Where was George Washington born?'' or ``Who was born in Virginia?'' A variant of this template-based strategy is employed by ELIZA, the therapist-like chatterbot developed in 1966 by Weizenbaum. More recent work by Ali et al. (2010) relies on sentence classification and syntax parsing before applying transformations to generate questions.  Wang et al. (2008) use a similar transformation-based procedure in the domain of medical texts. While template methods produce syntactically correct questions that are more appropriate for applications like dialogue, it is not clear that they produce deeper semantics than the simpler Cloze procedure. 

In 2010, a Question Generation Shared Task and Evaluation Challenge (QGSTEC) was announced, leading to an uptick of interest on the problem and a set of more sophisticated methods. QGSTEC included two tasks: QG from sentences, and QG from paragraphs. In both cases, the questions generated by the five contestants were evaluated manually on the basis of relevance, question type, correctness, ambiguity, and variety. This human-dependent evaluation unfortunately restricts the long term impact of QGSTEC, since new methods cannot be reliably compared with the baselines set in the contest. Nonetheless, the challenge succeeded in bolstering work on QGSTEC. Ali et al. entered a straightforward syntax-based method using Part of Speech tagging and Named Entity analysis. Pal et al. took a similar approach with an additional focus on clausal boundaries. Yao \& Zhang broached semantics directly by converting inputs into Minimal Recursion Semantics representations and then applying transformations into questions. Varga \& Ha modified an existing multiple choice question generator to fit the specification of the task. Mannem et al., the only team to attempt the paragraph to question subtask, used predicate argument structures to generate many more questions than required, and then ranked the candidate questions to select the final output. 

The \textit{overgenerating transformations and ranking} of Mannem et al. has been championed by Heilman, who completed his dissertation on the subject in 2011. The paradigm is a promising approach to QG, since it separates the task of generating all syntactically possible questions from the task of selecting semantically valid and useful questions. In Heilman's work, the overgeneration step is accomplished through syntax pattern matching and manually encoded transformation rule applications. Ranking is then performed using a variety natural language features. The ranking step is shown to double the acceptability of generated questions, as defined by human judges. The dataset tested is larger than that of QGSTEC, but still strictly limited by the dependency on human input.

\section{QG Ranking via Binary Classification of Coherence}

Overgenerating transformations and ranking is a promising approach to QG, but previous efforts have been severely limited by the requirement of manual human question labeling to train the ranking function. The standard procedure has been to overgenerate transformations, solicit humans to label the results as valid or invalid, and then train a ranking function that is optimized to award high rankings to valid questions. The theory is that this ranking function can subsequently be applied to other datasets of unlabeled questions generated via transformations or other procedures. Ranking according to statistical analyses of standard natural language features has been shown to double the percentage of valid questions present in the top 20\% ranked generated questions. While this result demonstrates the potential of ranking, it leaves significant room for improvement, as 50-60\% of invalid questions remain in the output. Ranking functions would presumably benefit from vastly more data to train on, but such human-labelled datasets remain scarce.

Here I propose that the ranking problem should be further decoupled from the overgeneration problem and recast in the context of binary classification. I start with a large dataset of questions written by humans. Such datasets are far more readily available than the ones previously used for training a rank function (e.g. see DATASETS). I assume that these questions are \textit{coherent}; presumably they would be marked as semantically and syntactically valid by human labelers, since they were written by humans. I then consider a set of \textit{mutated} questions, which are derived by applying lexical substitutions to questions in the original dataset. These mutated questions are assumedly incoherent, i.e. they likely contain syntactic or semantic errors that would solicit an invalid label from a human judge. Next I train a binary classifier on the full set of original and mutated questions, where the classes correspond to coherence. This trained binary classifier can then be used in place of the ranking functions previously described.

This general setup could be applied using any method for binary classification of natural language samples. In this chapter, I restrict focus by considering only three binary classifiers: Naive Bayes (NB), Logistic Regression (LR), and Convolutional Neural Networks (CNN). In each case, only unigram features are considered. NB and LR, both probabilistic models, use bag-of-words representations, meaning that no positional information about words is encoded in the input. For these two models, questions are represented as sparse vectors $\bold{x}$, where $x_i = 1$ if word $i \in \mathcal{V}$ is in the question. Thus training inputs are $\{\bold{x_i}, c_i\}$, where $c_i \in \{0, 1\}$ represents the coherence class. In contrast, a CNN is able to capture some positional information by learning features over windows of its input, which maintains the positional encoding of the text. Inputs to the CNN are ...... COMING SOON ..... NB and LR benefit from simplicity and training efficiency, while CNN pays for better results with a longer training time. I elaborate on each of the three models below.

\subsection{Naive Bayes}

Naive Bayes is a generative model based on the assumption that features are class-conditionally independent. The model estimates $p(c_i | x_i)$ by calculating maximum likelihood estimates for $p(x_i = 1 | c_i)$ and $p(c_i)$, assuming a constant $p(x_i)$, and then applying Bayes' Rule. Using the assumption that class-conditional feature probabilities and the prior class probabilities follow a multinomial distribution, it is possible to derive maximum likelihood estimates as follows:
\begin{center}
\begin{equation}
p(c_i = k) = \frac{N_k}{N}
\end{equation}
\begin{equation}
p(x_i = 1 | c_i = k) \propto \sum_{i=1}^N x_i\cdot 1(c_i = k)
\end{equation}
\end{center}
where $N_k = \sum_{i=1}^N 1(c_i = k)$ for $k = 0, 1$. To handle zero counts, I use Laplace smoothing, adding $\alpha$ to every recorded count.

\subsection{Logistic Regression}

Logistic Regression is a discriminative model that directly estimates the target probabilities $p(x_i = 1 | c_i)$. These probabilities are modeled as a nonlinear activation function, namely a sigmoid, applied to a linear function of the inputs, i.e. $\bold{y_i} = \sigma(\bold{x_i}W + b)$ where $\theta = (W, b)$ are the parameters of the model. The goal of LR is to find parameters that maximize the likelihood of the observed data, or equivalently, minimize the negative log likelihood (NLL) $log(\prod_i p(\bold{y_i} | \bold{x_i}, \theta)) = \sum_i log(p(\bold{y_i} | \bold{x_i}, \theta)$. Since there is no closed-form solution that maximizes this likelihood, an optimization method must be used. I use Stochastic Gradient Descent (SGD) in the experiments described in this chapter. 

\subsection{Convolutional Neural Networks}

TODO

\section{Experiments}

\subsection{Overview}

The general method for QG proposed in this chapter consists of three phases. First, a binary classifier for question coherence is trained on a large dataset consisting of real human-generated questions and fake mutated questions. Next, new candidate questions are overgenerated according to syntactic pattern matching or another simple method. Finally, the trained classifier is then applied to this large set of questions. Those identified as coherent by the classifier are added to the output. In this section, I present experimental results on the first phase, showing that binary classifiers can be trained with significantly greater accuracy than previously described statistical ranking functions.

\subsection{Dataset}

Any large dataset of questions written by humans can be used for training the binary classifier. Ideally, the questions should cover several topics, spanning not only trivia but also questions that require higher levels of logical reasoning. Here I use the WikiAnswers Paraphrase Dataset, released by PEOPLE. The dataset was created by scraping WikiAnswers, a website where users post questions and answers about a plethora of topics, over the period of one month in 2012. There are over 2.5 million unique questions present in the dataset. Given limits on training time and computational resources, I use 10\% of the full dataset by selecting every tenth question from the original set. This subset is partitioned into 232,774 samples for training and 25,801 for testing. As described below, the total size of both training and test datasets are doubled with the introduction of mutated questions.

\subsection{Methods}

\subsubsection{Preprocessing: Creating Mutated Questions}

From each human-authored question in the training and test datasets, a mutated question was created as follows. Half of the words in the original question were selected for mutation. Each word selected for mutation was replaced by a word randomly chosen from the unigram distribution, as estimated according to the word frequencies in the original dataset. For example, ``where is the largest library in the world?'' was mutated to ``mother is the in library did the world?'' These parameters -- one mutated question per real question and half mutated words -- were selected arbitrarily and may be optimized in future work. With the final training and test datasets created, preprocessing concluded by converting the natural language sentences into sparse representations according to indices into the full vocabulary. 

\subsubsection{Training and Testing}

All three models were implemented in Torch. For Naive Bayes, Laplace Smoothing was used according to an exposed hyperparameter $\alpha$. Logistic Regression and the Convolutional Neural Network were trained for 50 epochs using Stochastic Gradient Descent with minibatching. The learning rate $\eta$, the $L_2$ regularization penalty $\lambda$, and the minibatch size $K$ were exposed as hyperparameters. For the CNN, the hidden layer size comprised an additional hyperparameter. For LR and CNN, Cross-Entropy loss with $L_2$ regularization was used to derive the gradient for SGD. 

\subsubsection{Hyperparameter Optimization}

\section{Results}

\section{Discussion}
