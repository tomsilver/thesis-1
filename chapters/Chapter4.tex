\begin{savequote}[75mm]
An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem.
\qauthor{John Tukey}
\end{savequote}
\chapter{Interview Question Generation}

\section{Introduction}

Applications of artificial intelligence are often reactive: the human poses a question, and the machine attempts an answer. The beginning of a Luna Game may be confounding for such a responsive machine, since the Interview Phase requires each player to provide questions without any prior input. As a test for AI, the Luna Rating System does not demand that a machine be able to ask ``good'' questions; Smarts Ratings are presumably only a function of responses. Researchers who wish to use LRS purely as a test may choose a simple interviewing scheme, such as selection from a fixed set of questions. However, for those who want to take a more holistically AI approach to the Luna Game, the Interview Phase offers an application of the intriguing \textit{Question Generation} (QG) task.

Question Generation demands more than the creation of arbitrary linguistically valid questions. The defining feature of QG is that the questions produced must pertain to a given topic, which is often formalized as a collection of natural language samples. A slightly stronger requirement, sometimes included to narrow the scope of the task, is that the generated questions must be directly answered within the provided text. These text samples may be relatively short, like a section of a Wikipedia article, or long, like the full Wikipedia corpus. They may be factual, like the encyclopedia examples, or fictional, like a short story derived from a simulated world. A method for QG must accept these samples as input and produce questions related to the text. The metric used to assess this output may also vary. A straightforward approach is to count the number of unique questions generated from the input text. This metric avoids the difficulty of defining question quality but subsequently provides no guarantees about the question's content. An alternative method relies on manual human input to determine the quality of each question. While potentially more meaningful, this metric can be costly, subjective, and vulnerable to human error. The challenge of defining a metric that is both semantically meaningful and efficiently evaluated is arguably the greatest barrier between QG and mainstream NLP research.

In this chapter, I present a novel approach to QG that can provide quality questions without human labeling. The method extends the \textit{overgenerating transformations and ranking} paradigm, which begins with the generation of a vast surplus of questions, and then refines the crop by ranking the best questions and selecting from the top. In this context, the limiting factor for progress is the ranking part of the problem; methods for generating semantics-agnostic questions from templates are abundant. Previous work on ranking has relied on human-labelled questions and thus has been limited by the size of available annotated datasets. Taking inspiration from C\&W and their principle of coherence, I provide a simple method for converting a dataset of unlabeled questions into an augmented dataset of real coherent questions and fake incoherent questions. With this dataset in hand, one can proceed with binary classification methods to train a classifier for question coherence. In particular, methods that require large amounts of training data, such as deep neural networks, become available to the QG task for the first time. 

\section{Related Work}

Question Generation is often motivated by the prospect of automated intelligent educational tools. A system capable of generating a set of reasonable questions from an academic text would be invaluable for learning and assessment, especially in the era of MOOCs and other online educational resources. Heilman's work, which I describe in more detail below, is primarily motivated by this educational potential. In the specific context of English language learning, Kunichika et al. (2004) create an interactive system that generates questions from a novice textbook and then adaptively presents a series of questions to students based on their previous responses. Other motivations for QG include assisting human questioning, participating in general dialogue, and providing synthetic datasets for the related task of Question Answering (see Chapter 5). Piwek et al. (2008) make a broad case for QG as a task of interest for AI and Computational Linguistics, advocating an ``open-minded approach... [towards] a new and hopefully soon burgeoning area of research.''

If surface-level questions suffice for an application, and questions need not be diverse in style, there are several simple QG options available. One coarse approach is the Cloze procedure, in which words from the input text are randomly replaced with blanks for the responder to complete. For example, the input text ``George Washington was born in Virginia'' might generate the question ``George Washington was born in $\rule{2cm}{0.05mm}$''. The next level of sophistication involves applying templates to text in search of specific syntax patterns, and then applying one of several predefined transformations accordingly. For example, the previous input text might be matched and transformed to the question ``Where was George Washington born?'' or ``Who was born in Virginia?'' A variant of this template-based strategy is employed by ELIZA, the therapist-like chatterbot developed in 1966 by Weizenbaum. More recent work by Ali et al. (2010) relies on sentence classification and syntax parsing before applying transformations to generate questions.  Wang et al. (2008) use a similar transformation-based procedure in the domain of medical texts. While template methods produce syntactically correct questions that are more appropriate for applications like dialogue, it is not clear that they produce deeper semantics than the simpler Cloze procedure. 

In 2010, a Question Generation Shared Task and Evaluation Challenge (QGSTEC) was announced, leading to an uptick of interest on the problem and a set of more sophisticated methods. QGSTEC included two tasks: QG from sentences, and QG from paragraphs. In both cases, the questions generated by the five contestants were evaluated manually on the basis of relevance, question type, correctness, ambiguity, and variety. This human-dependent evaluation unfortunately restricts the long term impact of QGSTEC, since new methods cannot be reliably compared with the baselines set in the contest. Nonetheless, the challenge succeeded in bolstering work on QGSTEC. Ali et al. entered a straightforward syntax-based method using Part of Speech tagging and Named Entity analysis. Pal et al. took a similar approach with an additional focus on clausal boundaries. Yao \& Zhang broached semantics directly by converting inputs into Minimal Recursion Semantics representations and then applying transformations into questions. Varga \& Ha modified an existing multiple choice question generator to fit the specification of the task. Mannem et al., the only team to attempt the paragraph to question subtask, used predicate argument structures to generate many more questions than required, and then ranked the candidate questions to select the final output. 

The \textit{overgenerating transformations and ranking} of Mannem et al. has been championed by Heilman, who completed his dissertation on the subject in 2011. The paradigm is a promising approach to QG, since it separates the task of generating all syntactically possible questions from the task of selecting semantically valid and useful questions. In Heilman's work, the overgeneration step is accomplished through syntax pattern matching and manually encoded transformation rule applications. Ranking is then performed using a variety natural language features. The ranking step is shown to double the acceptability of generated questions, as defined by human judges. The dataset tested is larger than that of QGSTEC, but still strictly limited by the dependency on human input.

\section{Question Generation as Binary Classification}

\section{Experiments}

\section{Results}

\section{Discussion}
