\begin{savequote}[75mm]
You are like my father in some ways.\\
WHAT RESEMBLANCE DO YOU SEE\\
You are not very aggressive but I think you don't want me to notice that.\\
WHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE
\qauthor{A conversation with ELIZA (from \citet{weizenbaum1966eliza})}
\end{savequote}
\chapter{Question Generation and Response}

\section{Introduction}

In this chapter, I describe how the Luna Rating System fits within the broader context of existing AI research. LRS invites direct applications of two subfields of Natural Language Processing: Question Generation (QG) and Question Answering (Q\&A). QG studies the automatic generation of questions, typically derived from natural language input. The Interview Phase of the Luna Game, in which players must compose original questions and reason about expected answers, is a straightforward application of QG. Q\&A is the complementary study of answering natural language questions automatically. The Response Phase, which requires each player to answer the questions of the other player, similarly corresponds to Q\&A. (The Guess Phase does not have an obvious corollary problem in existing research, but its structure suggests an intriguing new problem, which I characterize in Chapter $5$.) The large bodies of work on QG and Q\&A can be directly incorporated into LRS. Here I offer a brief review of both problems and discuss their applicability to LRS. 

\section{Question Generation}

\subsection{Problem Scope}

Question Generation, as the name suggests, is the study of automatically generating linguistically valid questions. In practice, the generated questions must pertain to a given topic, which is often defined implicitly by providing a collection of natural language samples as input to the generator. A slightly stronger requirement, sometimes included to narrow the scope of the task, is that the generated questions must be directly answered within the provided text \citep{rus2011question, heilman2011automatic}. These text samples may be relatively short, like a single sentence \citep{ali2010automatic, rus2011question}), or longer, like a paragraph \citep{mannem2010question} or a full natural language corpus \citep{heilman2011automatic}. They may be factual, like the encyclopedia examples, or fictional, like a short story derived from a simulated world \cite{weston2015towards}. These various scope constraints have resulted in a field that is somewhat fragmented. Nonetheless, questions generated by any means may be used for the purpose of LRS.
 
\subsection{Research Context}

Question Generation is often motivated by the prospect of automated intelligent educational tools \citep{graesser2005scaffolding, heilman2011automatic}. A system capable of generating a set of reasonable questions from an academic text would be invaluable for learning and assessment, especially in the era of MOOCs and other online educational resources. Heilman's work, which I describe in more detail below, is primarily motivated by this educational potential \citep{heilman2011automatic}. In the specific context of English language learning, \citet{kunichika2004automated} create an interactive system that generates questions from a novice textbook and then adaptively presents a series of questions to students based on their previous responses. \citet{xu2009automatic} provide a similar game-based system for learning Mandarin. Other motivations for QG include assisting human questioning, participating in general dialogue \citep{walker2001spot}, and providing synthetic datasets for the related task of Question Answering (see Chapter $5$). \citet{piwek2008generating} make a broad case for QG as a task of interest for AI and Computational Linguistics, advocating an ``open-minded approach... [towards] a new and hopefully soon burgeoning area of research."

The success of questions generated for the purpose of the Interview Phase may be measured according to the precision and accuracy with which the author may subsequently guess the respondent's Smarts Rating. In existing research on QG, the metric used to assess the quality of generated questions is varied. A straightforward approach used in recent work is to count the number of unique questions generated from the input text. This metric avoids the difficulty of defining question quality but subsequently provides no guarantees about the question's content. An alternative method relies on manual human input to determine the quality of each question. While potentially more meaningful, this metric can be costly, subjective, and vulnerable to human error. The challenge of defining a metric that is both semantically meaningful and efficiently evaluated is arguably the greatest barrier between QG and mainstream NLP research.

\subsection{Existing Methods}

If surface-level questions suffice for an application, and questions need not be diverse in style, there are several simple QG options available. One coarse approach is the Cloze procedure, in which words from the input text are randomly replaced with blanks for the responder to complete \citep{taylor1953cloze}. For example, the input text ``George Washington was born in Virginia'' might generate the question ``George Washington was born in $\rule{2cm}{0.05mm}$''. The next level of sophistication involves applying templates to text in search of specific syntax patterns, and then applying one of several predefined transformations accordingly. For example, the previous input text might be matched and transformed to the question ``Where was George Washington born?'' or ``Who was born in Virginia?'' \citep{gates2008automatically, kunichika2004automated, heilman2011automatic}. A variant of this template-based strategy is employed by ELIZA, the therapist-like chatterbot developed in $1966$ by Weizenbaum \citep{weizenbaum1966eliza}. Template-based methods may also be used to generate multiple choice questions \citep{mitkov2006computer}. Work by \citet{ali2010automatic} relies on sentence classification and syntax parsing before applying transformations to generate questions.  \citet{wang2007automatic} use a similar transformation-based procedure in the domain of medical texts. While template methods produce syntactically correct questions that are more appropriate for applications like dialogue, it is not clear that they produce deeper semantics than the simpler Cloze procedure. 

In $2010$, the Question Generation Shared Task and Evaluation Challenge (QGSTEC) was announced, leading to an uptick of interest on the problem and a set of more sophisticated methods \citep{rus2011question}. QGSTEC included two tasks: QG from sentences, and QG from paragraphs. In both cases, the questions generated by the five contestants were evaluated manually on the basis of relevance, question type, correctness, ambiguity, and variety. This human-dependent evaluation unfortunately restricts the long term impact of QGSTEC, since new methods cannot be reliably compared with the baselines set in the contest. Nonetheless, the challenge succeeded in bolstering work on QG. \citet{ali2010automatic} entered a straightforward syntax-based method using Part of Speech tagging and Named Entity analysis. \citet{pal2010qgstec} took a similar approach with an additional focus on clausal boundaries. \citet{yao2010question} broached semantics directly by converting inputs into Minimal Recursion Semantics representations and then applying transformations into questions. \citet{vargaha} modified an existing multiple choice question generator to fit the specification of the task. \citet{mannem2010question}, the only team to attempt the paragraph to question subtask, used predicate argument structures to generate many more questions than required, and then ranked the candidate questions to select the final output. 

The \textit{overgenerating transformations and ranking} of Mannem et al. has been championed by Heilman, who completed his dissertation on the subject in $2011$ \citep{heilman2010extracting, heilman2010good, heilman2011automatic}. The paradigm is a promising approach to QG, since it separates the task of generating all syntactically possible questions from the task of selecting semantically valid and useful questions. In Heilman's work, the overgeneration step is accomplished through syntax pattern matching and manually encoded transformation rule applications. Ranking is then performed using a variety natural language features. The ranking step is shown to double the acceptability of generated questions, as defined by human judges. The dataset tested is larger than that of QGSTEC, but still strictly limited by the dependency on human input.

\subsection{Applicability to Luna}

The use of text input in existing studies of QG has both advantages and disadvantages in the context of the Luna Game. On one hand, questions derived from text often have clear correct answers, which simplifies the task of assessing responses. On the other hand, there are many probing questions that could not be reasonably constructed on the basis of text. (For example, there are uncountably many arithmetic problems that have never been asked!) In this sense, the practical scope of QG forms a subset of the general problem suggested by the Interview Phase, but the ultimate aims of both are concordant. 

TODO


\section{Question Answering}

\subsection{Problem Scope}


The Response Phase of the Luna Game bears immediate resemblance to a subfield of Natural Language Processing known as Question Answering (Q\&A). Q\&A is defined as the general problem of automatically responding to any question posed in natural language \citep{andrenucci2005automated, hirschman2001natural}. The breadth of the problem can be a blessing and a curse. Such a broad definition effectively encompasses all of artificial intelligence; it is argued that the problem can only be solved by a machine with true general AI \citep{yampolskiy2013turing}. This completeness is what makes Q\&A an appropriate centerpiece in a test for AI. At the same time, the tremendous scope of the problem can be a barrier to progress. The lack of common structure between possible questions gives researchers little to exploit. Moreover, the task of assessing a candidate solution to Q\&A presents several challenges and ambiguities. How should test questions be selected from the extraordinarily large number of possible question topics and forms? How should answers be assessed, especially in the case that a question may be subjective, or have several equally valid answers? These difficult questions have discouraged research on the general Q\&A problem in favor of more narrow tasks.

In pursuit of tractability, researchers have explored a variety of restrictions on Q\&A. These restrictions may apply to question content, question format, or answer format. Question content may be limited by focusing on a fixed source of information that assumedly contains all answers. The size and structure of this source can greatly influence the difficulty of the task. In one extreme, the source might be all of Wikipedia in natural language form, with no further direction nor additional parsing provided. An easier source would be a structured knowledge base like Freebase, which organizes relational information in a very precise and predictable way \citep{bollacker2008freebase}. The source may also be small and question-specific, e.g. a reading comprehension task that supplies text samples and asks the reader to infer answers based only on the samples \citep{richardson2013mctest}. With a source established, question formats are often limited so that there is a clear single correct answer. For example, the questions may be true or false, multiple choice, or single word answers. The answers themselves may be further limited to natural language fragments, or even single words, that are lifted directly from the provided text. Each of these potential restrictions on Q\&A represents a tradeoff between tractability and generalizability. As the field progresses, the range of what is tractable has expanded, allowing for commensurate improvements on more general problems.

\subsection{Research Context}

The motivation for Question Answering is abundant and self-evident. Every problem in AI --- in all fields, in fact --- can be phrased as a question. Imagine a machine that could answer every possible question. It could be asked, for example, ``What are the answers to all possible questions, ordered by importance to humanity?'' Of course, practical research on Q\&A is driven by much more immediate ambitions. Q\&A is often presented within the context of the World Wide Web and restricted to factual questions \citep{cucerzan2005factoid, ravichandran2002learning, kwok2001scaling}. A web-based system capable of directly answering user queries would be the prize possession of a search engine company. Indeed, with the introduction of Knowledge Panels with search responses that are derived from the Knowledge Graph, Google is increasingly blurring the lines between Information Retrieval and Q\&A \citep{singhal2012introducing}. Another extrinsic, if toy motivation for Q\&A is the television trivia game Jeopardy!, which inspired IBM's DeepQ\&A team to develop Watson, perhaps the most famous Q\&A system to date \citep{ferrucci2012introduction}. Additionally, personal assistant technologies, like the DARPA PAL project (which later became Apple's Siri) and the Amazon Echo, all rely on Q\&A for their core functionality \citep{aron2011innovative, tsiao2007natural}. The corporate origin of each of these examples is representative; there is enormous product-driven demand for progress on Q\&A.

\subsection{Existing Methods}

Q\&A has been studied consistently for over half a century. The majority of work on the subject falls into one of three categories: factoid Q\&A from structured knowledge bases, factoid Q\&A from free text, and non-factoid Q\&A. Given a structured knowledge base, i.e. a list of logical predicates, Q\&A essentially reduces to the subproblem of mapping natural language to queries, either explicitly or with the addition of a latent term \citep{yao2014information, zelle1996learning}. With multiple possible answers, an additional selection step is required, which usually involves a ranking function similar to those used for Question Generation. Factoid Q\&A from free text cannot take advantage of structured relations, and thus has the additional burden of parsing text from a natural language corpus in search of relevant information \citep{ravichandran2002learning}. While this added challenge is considerable, these algorithms typically also have access to significantly more data than their knowledge base oriented counterparts \citep{brill2001data, hermann2015teaching}. This setup makes the factoid Q\&A from free text especially appropriate for deep learning techniques. Both types of factoid Q\&A leave open the possibility of training an algorithm on an external dataset of facts before the questions are asked. In contrast, non-factoid Q\&A forces an algorithm to discover answers to questions on an ad-hoc basis \citep{soricut2004automatic}. Non-factoid Q\&A typically includes a fictional story as part of the prompt, and then ask a question which has an answer that can only be inferred from the story. While the information retrieval portion of the task is somewhat simplified, the challenges of automatic reasoning and inference are brought to the fore.

Another distinguishing feature among approaches to Q\&A is the extent to which questions and answers are parsed into intermediate representations. Recent efforts have attempted to learn mappings directly from questions to answers without forming any representations in between. Before the rise of deep learning, such efforts would have been unthinkable due to the incredibly large space of possible questions and answers in any practical domain \citep{hirschman2001natural}. For example, Figure \ref{fig:NLPQA}, reproduced from \citep{androutsopoulos1995natural}, provides an example of how an end-to-end system for Q\&A was divided into modules in $1995$. The system includes modules for syntax parsing, semantic rule application, database querying, and output generation, each which must be separately addressed. A $2001$ review by Burger et al. divides Q\&A even further into $12$ subproblems, including Question Classes, Question Processing, Context, Data Sources, Answer Extraction, and Answer Formulation among others \citep{burger2001issues}. The current state of the art suggests that this level of granularity does not necessarily lead to better performance.  

\begin{figure}[h]
\centerline{%
\includegraphics[width=0.5\textwidth]{figures/NLPQA.png}}%
\caption{Architecture of a typical Natural Language system for Question Answering, from \citep{androutsopoulos1995natural}. The structure could be used in full or in part as a template for approaching the Response Problem.}
\label{fig:NLPQA}
\end{figure}

In $1999$, a Q\&A task was added to the Text Retrieval Conference (TREC), an ongoing series of workshops in information retrieval that provides centralized benchmarks for many similar problems \citep{voorhees1999trec}. The dataset used for the task consists of $200$ factoid short-answer questions, such as ``How many calories are there in a Big Mac?'', and provides a natural language corpus of newspaper articles and similar archives that somewhere contain the answers. The answers provided by the algorithms are assessed by human judges for validity, representing the same bottleneck of the Question Generation task discussed above. Nonetheless, the Q\&A TREC task, which was repeated every year from $1999$ to $2007$, enjoyed far more attention than the analogous QG task has thus far \citep{dang2007overview}. The datasets from the task have also been utilized consistently in subsequent work and have been used as a benchmark to establish progress. Indeed, the concentration of $21$st century Q\&A research around the factoid free text subproblem is likely due in part to the prominence of the TREC task \citep{hirschman2001natural}.

Recent work by Facebook AI could potentially serve as an epicenter for research on the non-factoid subproblem. In a paper titled \textit{Towards AI Complete Question Answering: A Set of Prerequisite Toy Tasks}, Weston et al. define $20$ simplified non-factoid question answering subtasks, forming the bAbI task \citep{weston2015towards}. The subtasks are designed to strip away many of the superfluous complexities of naturally occurring text, instead focusing on core concepts one-by-one. Following the non-factoid question paradigm discussed above, questions are presented with a collection of statements containing the desired answer. For example, the simplest type of question is the Single Supporting Fact, in which the answer may be derived directly from a single provided statement. All questions in bAbI are based on a simulated world involving several agents and objects with various possible actions. In relying on a simple simulation, as did Winograd in earlier work \citep{winograd1971procedures}, bAbI is able to provide an ideal amount of unpredictability while still keeping the task focused on specific types of questions. The bAbI task has already inspired advances in deep learning approaches to Q\&A, including the notable introduction of Memory Networks \citep{sukhbaatar2015weakly}.

\subsection{Applicability to Luna}

TODO



\section{Conclusion}

This review of Question Generation and Question Answering demonstrates that the Luna Rating System does not exist in isolation from existing AI research. Rather, LRS can be seamlessly integrated into ongoing research on both of these problems, Q\&A in particular. The recent work presented here indicates research interest not only in Q\&A, but also in methods to steer research on the problem towards generality. The fact that a mainstream publication includes ``Towards AI-Complete Question Answering'' in its title is evidence of this trend. The Luna Rating System offers another path towards AI-Complete Question Answering --- a path that complements bAbI and requires no deviation from the current trajectory of the research community.

%\section{Response Phase: an Application of Question Answering}
%
%\subsection{Scope of Question Answering}
%
%The Response Phase of the Luna Game bears immediate resemblance to a subfield of Natural Language Processing known as Question Answering (Q\&A). Q\&A is defined as the general problem of automatically responding to any question posed in natural language \citep{andrenucci2005automated, hirschman2001natural}. The breadth of the problem can be a blessing and a curse. Such a broad definition effectively encompasses all of artificial intelligence; it is argued that the problem can only be solved by a machine with true general AI \citep{yampolskiy2013turing}. This completeness is what makes Q\&A an appropriate centerpiece in a test for AI. At the same time, the tremendous scope of the problem can be a barrier to progress. The lack of common structure between possible questions gives researchers little to exploit. Moreover, the task of assessing a candidate solution to Q\&A presents several challenges and ambiguities. How should test questions be selected from the extraordinarily large number of possible question topics and forms? How should answers be assessed, especially in the case that a question may be subjective, or have several equally valid answers? These difficult questions have discouraged research on the general Q\&A problem in favor of more narrow tasks.
%
%In pursuit of tractability, researchers have explored a variety of restrictions on Q\&A. These restrictions may apply to question content, question format, or answer format. Question content may be limited by focusing on a fixed source of information that assumedly contains all answers. The size and structure of this source can greatly influence the difficulty of the task. In one extreme, the source might be all of Wikipedia in natural language form, with no further direction nor additional parsing provided. An easier source would be a structured knowledge base like Freebase, which organizes relational information in a very precise and predictable way \citep{bollacker2008freebase}. The source may also be small and question-specific, e.g. a reading comprehension task that supplies text samples and asks the reader to infer answers based only on the samples \citep{richardson2013mctest}. With a source established, question formats are often limited so that there is a clear single correct answer. For example, the questions may be true or false, multiple choice, or single word answers. The answers themselves may be further limited to natural language fragments, or even single words, that are lifted directly from the provided text. Each of these potential restrictions on Q\&A represents a tradeoff between tractability and generalizability. As the field progresses, the range of what is tractable has expanded, allowing for commensurate improvements on more general problems.
%
%\subsection{Previous Work on Question Answering}
%
%The motivation for Question Answering is abundant and self-evident. Every problem in AI --- in all fields, in fact --- can be phrased as a question. Imagine a machine that could answer every possible question. It could be asked, for example, ``What are the answers to all possible questions, ordered by importance to humanity?'' Of course, practical research on Q\&A is driven by much more immediate ambitions. Q\&A is often presented within the context of the World Wide Web and restricted to factual questions \citep{cucerzan2005factoid, ravichandran2002learning, kwok2001scaling}. A web-based system capable of directly answering user queries would be the prize possession of a search engine company. Indeed, with the introduction of Knowledge Panels with search responses that are derived from the Knowledge Graph, Google is increasingly blurring the lines between Information Retrieval and Q\&A \citep{singhal2012introducing}. Another extrinsic, if toy motivation for Q\&A is the television trivia game Jeopardy!, which inspired IBM's DeepQ\&A team to develop Watson, perhaps the most famous Q\&A system to date \citep{ferrucci2012introduction}. Additionally, personal assistant technologies, like the DARPA PAL project (which later became Apple's Siri) and the Amazon Echo, all rely on Q\&A for their core functionality \citep{aron2011innovative, tsiao2007natural}. The corporate origin of each of these examples is representative; there is enormous product-driven demand for progress on Q\&A.
%
%Q\&A has been studied consistently for over half a century. The majority of work on the subject falls into one of three categories: factoid Q\&A from structured knowledge bases, factoid Q\&A from free text, and non-factoid Q\&A. Given a structured knowledge base, i.e. a list of logical predicates, Q\&A essentially reduces to the subproblem of mapping natural language to queries, either explicitly or with the addition of a latent term \citep{yao2014information, zelle1996learning}. With multiple possible answers, an additional selection step is required, which usually involves a ranking function similar to those used for Question Generation. Factoid Q\&A from free text cannot take advantage of structured relations, and thus has the additional burden of parsing text from a natural language corpus in search of relevant information \citep{ravichandran2002learning}. While this added challenge is considerable, these algorithms typically also have access to significantly more data than their knowledge base oriented counterparts \citep{brill2001data, hermann2015teaching}. This setup makes the factoid Q\&A from free text especially appropriate for deep learning techniques. Both types of factoid Q\&A leave open the possibility of training an algorithm on an external dataset of facts before the questions are asked. In contrast, non-factoid Q\&A forces an algorithm to discover answers to questions on an ad-hoc basis \citep{soricut2004automatic}. Non-factoid Q\&A typically includes a fictional story as part of the prompt, and then ask a question which has an answer that can only be inferred from the story. While the information retrieval portion of the task is somewhat simplified, the challenges of automatic reasoning and inference are brought to the fore.
%
%Another distinguishing feature among approaches to Q\&A is the extent to which questions and answers are parsed into intermediate representations. Recent efforts have attempted to learn mappings directly from questions to answers without forming any representations in between. Before the rise of deep learning, such efforts would have been unthinkable due to the incredibly large space of possible questions and answers in any practical domain \citep{hirschman2001natural}. For example, Figure \ref{fig:NLPQA}, reproduced from \citep{androutsopoulos1995natural}, provides an example of how an end-to-end system for Q\&A was divided into modules in $1995$. The system includes modules for syntax parsing, semantic rule application, database querying, and output generation, each which must be separately addressed. A $2001$ review by Burger et al. divides Q\&A even further into $12$ subproblems, including Question Classes, Question Processing, Context, Data Sources, Answer Extraction, and Answer Formulation among others \citep{burger2001issues}. The current state of the art suggests that this level of granularity does not necessarily lead to better performance.  
%
%\begin{figure}[h]
%\centerline{%
%\includegraphics[width=0.5\textwidth]{figures/NLPQA.png}}%
%\caption{Architecture of a typical Natural Language system for Question Answering, from \citep{androutsopoulos1995natural}. The structure could be used in full or in part as a template for approaching the Response Problem.}
%\label{fig:NLPQA}
%\end{figure}
%
%In $1999$, a Q\&A task was added to the Text Retrieval Conference (TREC), an ongoing series of workshops in information retrieval that provides centralized benchmarks for many similar problems \citep{voorhees1999trec}. The dataset used for the task consists of $200$ factoid short-answer questions, such as ``How many calories are there in a Big Mac?'', and provides a natural language corpus of newspaper articles and similar archives that somewhere contain the answers. The answers provided by the algorithms are assessed by human judges for validity, representing the same bottleneck of the Question Generation task discussed above. Nonetheless, the Q\&A TREC task, which was repeated every year from $1999$ to $2007$, enjoyed far more attention than the analogous QG task has thus far \citep{dang2007overview}. The datasets from the task have also been utilized consistently in subsequent work and have been used as a benchmark to establish progress. Indeed, the concentration of $21$st century Q\&A research around the factoid free text subproblem is likely due in part to the prominence of the TREC task \citep{hirschman2001natural}.
%
%Recent work by Facebook AI could potentially serve as an epicenter for research on the non-factoid subproblem. In a paper titled \textit{Towards AI Complete Question Answering: A Set of Prerequisite Toy Tasks}, Weston et al. define $20$ simplified non-factoid question answering subtasks, forming the bAbI task \citep{weston2015towards}. The subtasks are designed to strip away many of the superfluous complexities of naturally occurring text, instead focusing on core concepts one-by-one. Following the non-factoid question paradigm discussed above, questions are presented with a collection of statements containing the desired answer. For example, the simplest type of question is the Single Supporting Fact, in which the answer may be derived directly from a single provided statement. All questions in bAbI are based on a simulated world involving several agents and objects with various possible actions. In relying on a simple simulation, as did Winograd in earlier work \citep{winograd1971procedures}, bAbI is able to provide an ideal amount of unpredictability while still keeping the task focused on specific types of questions. The bAbI task has already inspired advances in deep learning approaches to Q\&A, including the notable introduction of Memory Networks \citep{sukhbaatar2015weakly}.
%
%\subsection{Question Answering: The Way Forward}
%
%As evidenced by its inclusion in TREC, Q\&A has long been viewed as an extension of information retrieval \citep{kolomiyets2011survey}. Classical information retrieval returns a document containing relevant information in response to a query. Q\&A accepts the same type of query and returns a passage or parsed passage of relevant text from a similar source. The introduction of bAbI, with its non-factoid questions based on a simple simulated world, signifies a shift in the conception of Q\&A. Research on the problem is now reaching towards sophisticated reasoning in small worlds, as opposed to simple reasoning in large worlds. This shift is a critical step in the pursuit of AI-complete Q\&A. Simple factoid questions represent a very limited subset of the questions that a true AI will be expected to answer. As progress on the existing bAbI tasks continue, it will be expedient to gradually add more sophisticated tasks to continue to guide research.
%
%Work on bAbI has already produced algorithms that are able to handle an impressive range of question types pertaining to the small simulated world. Information retrieval efforts continue in parallel, making it possible to answer simple questions over increasingly large sets of semantics. A critical question for the future of Q\&A is how to close the gap between these two problems. One approach would be to gradually scale solutions to bAbI tasks to larger worlds. A mirrored method would be to increase the level of sophistication in the questions that information retrieval algorithms are able to answer. A third, and potentially most promising strategy, would be to explicitly bridge the divide while maintaining the distinction between the two problems. In this scenario, a machine could learn relations between a very limited number of entities, in the spirit of bAbI. It could then learn about a vast number of entities without learning new relations using traditional information retrieval. Finally, a mapping between the entities in the bAbI task and the retrieved entities could be learned, and new relations between these retrieved entities could be inferred. 

